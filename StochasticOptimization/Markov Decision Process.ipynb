{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "n_states = 2\n",
    "P = Array{Float64, 3}(undef, n_actions, n_states, n_states)\n",
    "# Transition probability matrices for actions 1, 2\n",
    "P[1, :, :] = [\n",
    "    0.7 0.3\n",
    "    0.4 0.6\n",
    "]\n",
    "\n",
    "P[2, :, :] = [\n",
    "    0.1 0.9\n",
    "    0.8 0.2\n",
    "]\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = Array{Float64, 3}(undef, n_actions, n_states, n_states)\n",
    "# Transition reward matrices for actions 1, 2\n",
    "R[1, :, :] = [\n",
    "    11 -4\n",
    "    -14 6\n",
    "]\n",
    "\n",
    "R[2, :, :] = [\n",
    "    45 80\n",
    "    1 -23\n",
    "]\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the 4 possible policies.\n",
    "mu = (2, 1) # on state 1 perform action 2, on state 2 perform action 1\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 0.1  0.9\n",
       " 0.4  0.6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build the transition probability matrix for this policy\n",
    "Pmu = [\n",
    "    P[mu[1], 1, :]'\n",
    "    P[mu[2], 2, :]'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  45.0  80.0\n",
       " -14.0   6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build the transition reward matrix for this policy\n",
    "Rmu = [\n",
    "    R[mu[1], 1, :]'\n",
    "    R[mu[2], 2, :]'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "[0.7 0.3; 0.4 0.6]\n",
      "[11.0 -4.0; -14.0 6.0]\n",
      "\n",
      "(1, 2)\n",
      "[0.7 0.3; 0.8 0.2]\n",
      "[11.0 -4.0; 1.0 -23.0]\n",
      "\n",
      "(2, 1)\n",
      "[0.1 0.9; 0.4 0.6]\n",
      "[45.0 80.0; -14.0 6.0]\n",
      "\n",
      "(2, 2)\n",
      "[0.1 0.9; 0.8 0.2]\n",
      "[45.0 80.0; 1.0 -23.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print for all possible policies, the transition probability matrix and the\n",
    "# transition reward matrix\n",
    "for mu in ((1,1), (1,2), (2,1), (2,2))\n",
    "    println(mu)\n",
    "    println([\n",
    "        P[mu[1], 1, :]'\n",
    "        P[mu[2], 2, :]'\n",
    "    ])\n",
    "    println([\n",
    "        R[mu[1], 1, :]'\n",
    "        R[mu[2], 2, :]'\n",
    "    ])\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.499999999999999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected immediate reward for state i, given action a\n",
    "r_bar(i, a) = sum(P[a, i, j] * R[a, i, j] for j in 1:n_states)\n",
    "# expected immediate reward for the state 1, given action 1\n",
    "r_bar(1, 1) # 0.7 * 11 + 0.3 * -0.4 = 7.7 - 1.2 = 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.30769230769230815\n",
       " 0.6923076923076932"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stationary distribution of chain Pmu\n",
    "PImu = (Pmu^100)[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.153846153846185"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average reward for policy mu\n",
    "sum(PImu[i] * r_bar(i, mu[i]) for i in 1:n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       "  45.0  80.0\n",
       " -14.0   6.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    Rμ = [\n",
    "        R[mu[1], 1, :]'\n",
    "        R[mu[2], 2, :]'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ = (1, 1), ρμ = 2.86\n",
      "μ = (1, 2), ρμ = 3.69\n",
      "μ = (2, 1), ρμ = 22.15\n",
      "μ = (2, 2), ρμ = 33.99\n"
     ]
    }
   ],
   "source": [
    "# Exhaustive enumeration to find the best policy O(|A|^|S|)\n",
    "for μ in ((1,1), (1,2), (2,1), (2,2))\n",
    "    Pμ = vcat([P[μ[i], i, :]' for i in 1:n_states]...)\n",
    "    Rμ = vcat([R[μ[i], i, :]' for i in 1:n_states]...)\n",
    "    \n",
    "    Πμ = (Pμ^100)[1, :]\n",
    "    \n",
    "    ρμ = sum(Πμ[i] * r_bar(i, μ[i]) for i in 1:n_states)\n",
    "    \n",
    "    println(\"μ = $(μ), ρμ = $(round(ρμ, digits=2))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ = (1, 1), ρμ = 5.83\n",
      "μ = (1, 2), ρμ = 5.64\n",
      "μ = (2, 1), ρμ = 10.56\n",
      "μ = (2, 2), ρμ = 9.67\n"
     ]
    }
   ],
   "source": [
    "# Exhaustive enumeration example A\n",
    "n_actions = 2\n",
    "n_states = 2\n",
    "P = Array{Float64, 3}(undef, n_actions, n_states, n_states)\n",
    "P[1, :, :] = [\n",
    "    0.7 0.3\n",
    "    0.4 0.6\n",
    "]\n",
    "P[2, :, :] = [\n",
    "    0.9 0.1\n",
    "    0.2 0.8\n",
    "]\n",
    "R = Array{Float64, 3}(undef, n_actions, n_states, n_states)\n",
    "R[1, :, :] = [\n",
    "    6 -5\n",
    "    7 12\n",
    "]\n",
    "R[2, :, :] = [\n",
    "    10 17\n",
    "    -14 13\n",
    "]\n",
    "for μ in ((1,1), (1,2), (2,1), (2,2))\n",
    "    Pμ = vcat([P[μ[i], i, :]' for i in 1:n_states]...)\n",
    "    Rμ = vcat([R[μ[i], i, :]' for i in 1:n_states]...)\n",
    "    \n",
    "    Πμ = (Pμ^100)[1, :]\n",
    "    \n",
    "    ρμ = sum(Πμ[i] * r_bar(i, μ[i]) for i in 1:n_states)\n",
    "    \n",
    "    println(\"μ = $(μ), ρμ = $(round(ρμ, digits=2))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ = (1, 1), 5.8112\n",
      "μ = (1, 1), 5.7742\n",
      "μ = (2, 2), 9.6081\n",
      "μ = (2, 2), 9.7085\n"
     ]
    }
   ],
   "source": [
    "# Calculate rho_mu from its definition (by simulation).\n",
    "# See that the initial value doesnt count very much\n",
    "\n",
    "# Sigma reward\n",
    "function simulate(P, R, i0)\n",
    "    i = i0\n",
    "    acc = 0\n",
    "    for _ in 1:10_000\n",
    "        j = sample(1:2, Weights(P[i, :]))\n",
    "        acc += R[i, j]\n",
    "        i = j\n",
    "    end\n",
    "    return acc / 10_000\n",
    "end\n",
    "\n",
    "println(\"μ = (1, 1), $(simulate(P[1, :, :], R[1, :, :], 1))\")\n",
    "println(\"μ = (1, 1), $(simulate(P[1, :, :], R[1, :, :], 2))\")\n",
    "println(\"μ = (2, 2), $(simulate(P[2, :, :], R[2, :, :], 1))\")\n",
    "println(\"μ = (2, 2), $(simulate(P[2, :, :], R[2, :, :], 2))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 1], [0.0, 10.428])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Policy Iteration Algorithm\n",
    "\n",
    "function policy_iteration()\n",
    "    k = 1\n",
    "    mu = [1, 1] # an arbitrary starting policy\n",
    "    \n",
    "    # how did I calculate this 10.43\n",
    "    h = [0., 10.43] # arbitrary starting value function\n",
    "    rho = 5.83 # arbitrary starting average reward\n",
    "    \n",
    "    # let's do 1 iteration\n",
    "    # policy evaluation\n",
    "    for i in 2:n_states\n",
    "        h[i] = r_bar(i, mu[i]) - rho + sum(P[mu[i], i, j] * h[j] for j in 1:n_states)\n",
    "    end\n",
    "\n",
    "    # policy improvement\n",
    "    for i in 1:n_states\n",
    "        mu[i] = argmax([r_bar(i, a) + sum(P[a, i, j]*h[j] for j in 1:n_states) for a in 1:n_actions])\n",
    "    end\n",
    "    \n",
    "    # h doesnt work...\n",
    "    mu, h\n",
    "end\n",
    "\n",
    "policy_iteration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Andrea): implement value iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
