{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module MyFiniteDiff\n",
    "using LinearAlgebra\n",
    "function _e(T, i, n)\n",
    "    v = zeros(T, n)\n",
    "    v[i] = one(T)\n",
    "    return v\n",
    "end\n",
    "\n",
    "\"Find the numerical gradient of f : R^2 -> R using forward or centered differences\"\n",
    "function grad_finite_diff(x, f, h, type_)   \n",
    "    n = size(x, 1)\n",
    "    grad = similar(x)\n",
    "    \n",
    "    fx = f(x)\n",
    "    \n",
    "    e(i) = _e(eltype(x), i, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        if type_ == :fw\n",
    "            grad[i] = (f(x + h*e(i)) - fx) / h\n",
    "        end\n",
    "        if type_ == :c\n",
    "            grad[i] = (f(x + h*e(i)) - f(x - h*e(i))) / 2h\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return grad\n",
    "end\n",
    "\n",
    "function hess_finite_diff(x, f, h)\n",
    "    n = size(x, 1)\n",
    "    grad = similar(x, n, n)\n",
    "    \n",
    "    e(i) = _e(eltype(x), i, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        for j in 1:n\n",
    "            # Just calculate the upper triangle\n",
    "            if i < j\n",
    "                grad[i, j] = (\n",
    "                    f(x + h*e(i) + h*e(j)) - f(x + h*e(i)) - f(x + h*e(j)) + f(x)\n",
    "                ) / (h^2)\n",
    "            end\n",
    "            if i == j\n",
    "                grad[i, i] = (\n",
    "                    f(x + h*e(i)) - 2*f(x) + f(x - h*e(i))\n",
    "                ) / (h^2)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # And then copy the upper part into the lower part\n",
    "    return Symmetric(grad)\n",
    "end\n",
    "\n",
    "function jacobian_finite_diff(F, x, h, type_)\n",
    "    n = size(x, 1)\n",
    "    jac = similar(x, n, n)\n",
    "    \n",
    "    e(i) = _e(eltype(x), i, n)\n",
    "    \n",
    "    Fx = F(x)\n",
    "    \n",
    "    \n",
    "    for i in 1:n\n",
    "        if type_ == :fw\n",
    "            ∇ₓᵢF = (F(x + h * e(i)) - Fx) / h\n",
    "            jac[:, i] = ∇ₓᵢF\n",
    "        end\n",
    "         if type_ == :c\n",
    "            ∇ₓᵢF = (F(x + h * e(i)) - F(x - h * e(i))) / 2h\n",
    "            jac[:, i] = ∇ₓᵢF\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return jac\n",
    "end\n",
    "\n",
    "function newton_method_backtrack_finite_diff(x0, f, kmax, tollgrad, alpha0, c1, rho, btmax, FDgrad, FDhess)\n",
    "    gradf(x) = grad_finite_diff(x, f, sqrt(eps()), FDgrad)\n",
    "    \n",
    "    hessf(x) = error(\"Undefined hessf\")\n",
    "    # TODO(Andrea): finish\n",
    "    if FDhess == :c\n",
    "        hessf = x -> hess_finite_diff(x, f, 5e-2)\n",
    "    elseif FDhess == :Jfw\n",
    "        hessf = x -> MyFiniteDiff.jacobian_finite_diff(\n",
    "            (x -> MyFiniteDiff.grad_finite_diff(x, f, sqrt(eps()), :fw)),\n",
    "            x,\n",
    "            5e-2,\n",
    "            :fw\n",
    "        )\n",
    "    elseif FDhess == :Jc\n",
    "        hessf = x -> MyFiniteDiff.jacobian_finite_diff(\n",
    "            (x -> MyFiniteDiff.grad_finite_diff(x, f, sqrt(eps()), :fw)),\n",
    "            x,\n",
    "            5e-2,\n",
    "            :c\n",
    "        )\n",
    "    elseif FDhess == :MF\n",
    "         # Matrix free, only defines matrix-hessian product\n",
    "         error(\"Matrix-free method is unimplemented!\")\n",
    "    else\n",
    "        error(\"Unknown method for Hessian finite differences\")\n",
    "    end\n",
    "\n",
    "    \n",
    "    k = 1\n",
    "    xk = x0\n",
    "    xseq = Vector{typeof(x0)}()\n",
    "    btseq = Vector{Int}()\n",
    "    gradfk = gradf(xk)\n",
    "    push!(xseq, xk)\n",
    "    \n",
    "    while k < kmax\n",
    "        if norm(gradfk) < tollgrad\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        pk = hessf(xk) \\ -gradfk\n",
    "        \n",
    "        # Backtracking\n",
    "        alpha = alpha0\n",
    "        bt = 1\n",
    "        while f(xk + alpha * pk) > (f(xk) + c1 * alpha * gradf(xk)'*pk) && bt < btmax\n",
    "            alpha = rho * alpha\n",
    "            bt = bt + 1\n",
    "        end\n",
    "        push!(btseq, bt)\n",
    "        \n",
    "        xk = xk + alpha * pk\n",
    "        k = k + 1\n",
    "        gradfk = gradf(xk)\n",
    "        push!(xseq, xk)\n",
    "    end\n",
    "    \n",
    "    return xk, f(xk), gradfk, k, xseq, btseq\n",
    "end\n",
    "    \n",
    "end\n",
    "\n",
    "import Main.MyFiniteDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk could be obtained with an iterative method, for which only hessian-vector products are needed, thus avoiding building the whole hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert MyFiniteDiff.grad_finite_diff([3.], x -> x[1]^2, sqrt(eps()), :fw) ≈ [6]\n",
    "@assert MyFiniteDiff.grad_finite_diff([3.], x -> x[1]^2, sqrt(eps()), :c) ≈ [6]\n",
    "@assert MyFiniteDiff.grad_finite_diff([float(pi)], x -> sin(x[1]), sqrt(eps()), :fw) ≈ [-1]\n",
    "@assert MyFiniteDiff.grad_finite_diff([float(pi)], x -> sin(x[1]), sqrt(eps()), :c) ≈ [-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fails with sqrt(eps()). Big numerical cancellation problems\n",
    "@assert isapprox(MyFiniteDiff.hess_finite_diff([3., 2.], x -> x[1]^2 + x[2]^2, sqrt(eps())*norm([3., 2.])), [2. 0.; 0. 2.], atol=1.2)\n",
    "# much more precise with a bigger step\n",
    "@assert isapprox(MyFiniteDiff.hess_finite_diff([3., 2.], x -> x[1]^2 + x[2]^2, 5e-2), [2. 0.; 0. 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert MyFiniteDiff.jacobian_finite_diff(x -> [x[1]^2 + x[2]^2, 3*x[1] - 4*x[2]^2], [10, 10], sqrt(eps()), :fw) ≈ [20 20; 3 -80]\n",
    "@assert MyFiniteDiff.jacobian_finite_diff(x -> [x[1]^2 + x[2]^2, 3*x[1] - 4*x[2]^2], [10, 10], sqrt(eps()), :c) ≈ [20 20; 3 -80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Hessian as Jacobian of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice here how we have got even bigger numerical precision problems\n",
    "@assert isapprox(MyFiniteDiff.jacobian_finite_diff(\n",
    "    (x -> MyFiniteDiff.grad_finite_diff(x, x -> x[1]^2 + x[2]^2, sqrt(eps()), :fw)),\n",
    "    [3., 2.],\n",
    "    5e-2,\n",
    "    :fw\n",
    "), [2.0 0.0; 0.0 2.0], atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Netwon Method with numerical gradient and Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk = [-2.0861924161863305e-8, -1.1920972965678876e-8]\n",
      "fk = 5.000000000000001\n",
      "gradfk = [0.0, 0.0]\n",
      "k = 28\n",
      "xk = [-7.659289540317465e-9, -9.637928486669447e-9]\n",
      "fk = 5.0\n",
      "gradfk = [0.0, 0.0]\n",
      "k = 29\n",
      "xk = [-9.004309664913202e-9, -9.005858892366818e-9]\n",
      "fk = 5.0\n",
      "gradfk = [0.0, 0.0]\n",
      "k = 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(x) = x[1]^2 + 4*x[2]^2 + 5\n",
    "\n",
    "kmax = 1000\n",
    "tollgrad = 1e-12\n",
    "x0 = [1.0, 1.0]\n",
    "alpha0 = 1.5\n",
    "c1 = 1e-4\n",
    "rho = 0.8\n",
    "btmax = 50\n",
    "\n",
    "xk, fk, gradfk, k, xseq, btseq = MyFiniteDiff.newton_method_backtrack_finite_diff(x0, f1, kmax, tollgrad, alpha0, c1, rho, btmax, :fw, :c)\n",
    "@show xk\n",
    "@show fk\n",
    "@show gradfk\n",
    "@show k\n",
    "\n",
    "xk, fk, gradfk, k, xseq, btseq = MyFiniteDiff.newton_method_backtrack_finite_diff(x0, f1, kmax, tollgrad, alpha0, c1, rho, btmax, :fw, :Jfw)\n",
    "@show xk\n",
    "@show fk\n",
    "@show gradfk\n",
    "@show k\n",
    "\n",
    "xk, fk, gradfk, k, xseq, btseq = MyFiniteDiff.newton_method_backtrack_finite_diff(x0, f1, kmax, tollgrad, alpha0, c1, rho, btmax, :fw, :Jc)\n",
    "@show xk\n",
    "@show fk\n",
    "@show gradfk\n",
    "@show k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
